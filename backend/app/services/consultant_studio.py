"""
Consultant Studio Service - Enhanced three-path validation system
Implements: Validate Idea, Search Ideas, Identify Location
"""

import asyncio
import hashlib
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from app.models.consultant_activity import ConsultantActivity, ConsultantPath
from app.models.detected_trend import DetectedTrend
from app.models.trend_opportunity_mapping import TrendOpportunityMapping
from app.models.location_analysis_cache import LocationAnalysisCache, BusinessType
from app.models.idea_validation_cache import IdeaValidationCache
from app.models.opportunity import Opportunity

logger = logging.getLogger(__name__)

IDEA_CACHE_TTL_DAYS = 7

# State name to abbreviation mapping
from .location_utils import STATE_ABBREVIATIONS, normalize_state, validate_coordinates_in_state

def parse_city_state(location: str) -> tuple[str, Optional[str]]:
    """
    Parse a location string like 'Miami, Florida' into (city, state_abbrev).
    Returns (city, None) if state cannot be parsed.
    Uses centralized normalize_state from location_utils.
    """
    if not location:
        return location, None
    
    if ',' in location:
        parts = [p.strip() for p in location.split(',')]
        if len(parts) >= 2:
            city = parts[0]
            state_part = parts[1].strip()
            
            state_abbrev = normalize_state(state_part)
            return city, state_abbrev
    
    return location, None


class ConsultantStudioService:
    """Three-path validation system with dual AI architecture"""

    CACHE_TTL_DAYS = 30

    def __init__(self, db: Session):
        self.db = db

    def _get_cache_key(self, idea_description: str, context: Optional[Dict] = None) -> str:
        """Generate a cache key for validation results"""
        content = idea_description.lower().strip()
        if context:
            content += json.dumps(context, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

    def _get_cached_validation(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Get cached validation result from database if exists and not expired"""
        try:
            cached = self.db.query(IdeaValidationCache).filter(
                IdeaValidationCache.cache_key == cache_key,
                IdeaValidationCache.expires_at > datetime.utcnow()
            ).first()
            
            if cached:
                cached.hit_count += 1
                cached.updated_at = datetime.utcnow()
                self.db.commit()
                
                logger.info(f"DB cache hit for validation: {cache_key[:8]} (hits: {cached.hit_count})")
                return {
                    "success": True,
                    "idea_description": cached.idea_description,
                    "recommendation": cached.recommendation,
                    "online_score": cached.online_score,
                    "physical_score": cached.physical_score,
                    "pattern_analysis": cached.pattern_analysis or {},
                    "viability_report": cached.viability_report or {},
                    "similar_opportunities": cached.similar_opportunities or [],
                    "processing_time_ms": cached.processing_time_ms,
                }
        except Exception as e:
            logger.warning(f"Cache lookup failed: {e}")
            self.db.rollback()
        return None

    def _cache_validation(self, cache_key: str, idea_description: str, context: Optional[Dict], result: Dict[str, Any]):
        """Cache validation result to database with safe upsert handling"""
        try:
            existing = self.db.query(IdeaValidationCache).filter(
                IdeaValidationCache.cache_key == cache_key
            ).first()
            
            if existing:
                existing.recommendation = result.get("recommendation")
                existing.online_score = result.get("online_score")
                existing.physical_score = result.get("physical_score")
                existing.pattern_analysis = result.get("pattern_analysis")
                existing.viability_report = result.get("viability_report")
                existing.similar_opportunities = result.get("similar_opportunities")
                existing.processing_time_ms = result.get("processing_time_ms")
                existing.expires_at = datetime.utcnow() + timedelta(days=IDEA_CACHE_TTL_DAYS)
                existing.updated_at = datetime.utcnow()
                self.db.commit()
            else:
                new_cache = IdeaValidationCache(
                    cache_key=cache_key,
                    idea_description=idea_description,
                    business_context=context,
                    recommendation=result.get("recommendation"),
                    online_score=result.get("online_score"),
                    physical_score=result.get("physical_score"),
                    pattern_analysis=result.get("pattern_analysis"),
                    viability_report=result.get("viability_report"),
                    similar_opportunities=result.get("similar_opportunities"),
                    processing_time_ms=result.get("processing_time_ms"),
                    hit_count=0,
                    expires_at=datetime.utcnow() + timedelta(days=IDEA_CACHE_TTL_DAYS)
                )
                self.db.add(new_cache)
                try:
                    self.db.commit()
                except IntegrityError:
                    self.db.rollback()
                    logger.info(f"Cache entry already exists (race condition): {cache_key[:8]}")
                    return
            
            logger.info(f"Cached validation result: {cache_key[:8]}")
        except Exception as e:
            logger.warning(f"Failed to cache validation: {e}")
            self.db.rollback()

    async def validate_idea(
        self,
        user_id: int,
        idea_description: str,
        business_context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Path 1: Validate Idea - Online vs Physical decision engine
        Uses DeepSeek for pattern analysis + Claude for viability report
        OPTIMIZED: Parallel AI calls for faster response (DeepSeek + Claude run concurrently)
        """
        import time
        import asyncio
        start_time = time.time()
        
        cache_key = self._get_cache_key(idea_description, business_context)
        cached_result = self._get_cached_validation(cache_key)
        if cached_result:
            cached_result['from_cache'] = True
            cached_result['processing_time_ms'] = int((time.time() - start_time) * 1000)
            return cached_result
        
        try:
            similar_task = self._find_similar_opportunities(idea_description)
            viability_task = self._claude_viability_report_parallel(
                idea_description,
                business_context
            )
            pattern_task = self._deepseek_pattern_analysis_parallel(
                idea_description,
                business_context
            )
            
            similar_opportunities, viability_report, pattern_analysis = await asyncio.gather(
                similar_task,
                viability_task,
                pattern_task,
                return_exceptions=True
            )
            
            if isinstance(similar_opportunities, Exception):
                logger.warning(f"Similar opportunities search failed: {similar_opportunities}")
                similar_opportunities = []
            if isinstance(viability_report, Exception):
                logger.warning(f"Claude viability report failed: {viability_report}")
                viability_report = {}
            if isinstance(pattern_analysis, Exception):
                logger.warning(f"DeepSeek pattern analysis failed: {pattern_analysis}")
                pattern_analysis = {}
            
            online_score, physical_score = self._calculate_business_type_scores(
                pattern_analysis,
                business_context
            )
            
            if viability_report and online_score and physical_score:
                viability_report["confidence_score"] = min(95, max(60, (online_score + physical_score) // 2 + 20))
            
            recommendation = self._determine_recommendation(online_score, physical_score)
            
            processing_time = int((time.time() - start_time) * 1000)
            
            result = {
                "success": True,
                "idea_description": idea_description,
                "recommendation": recommendation,
                "online_score": online_score,
                "physical_score": physical_score,
                "pattern_analysis": pattern_analysis,
                "viability_report": viability_report,
                "similar_opportunities": [
                    {"id": o.id, "title": o.title, "score": o.feasibility_score or 0}
                    for o in similar_opportunities[:5]
                ],
                "processing_time_ms": processing_time,
                "from_cache": False,
            }
            
            self._cache_validation(cache_key, idea_description, business_context, result)
            
            await self._log_activity(
                user_id=user_id,
                session_id=session_id,
                path=ConsultantPath.validate_idea.value,
                action="idea_validation_complete",
                payload={"idea": idea_description[:200], "context": business_context},
                result_summary=f"Recommended: {recommendation}, Online: {online_score}, Physical: {physical_score}",
                ai_model_used="hybrid",
                processing_time_ms=processing_time,
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error validating idea: {e}")
            return {"success": False, "error": str(e)}

    async def search_ideas(
        self,
        user_id: int,
        filters: Dict[str, Any],
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Path 2: Search Ideas - Database exploration with trend detection
        Uses DeepSeek for trend detection + Claude for synthesis
        """
        import time
        start_time = time.time()
        
        try:
            opportunities = await self._search_opportunities(filters)
            
            trends = await self._detect_trends(opportunities, filters)
            
            synthesis = self._generate_quick_synthesis(opportunities, trends, filters)
            
            processing_time = int((time.time() - start_time) * 1000)
            
            result = {
                "success": True,
                "opportunities": [
                    {
                        "id": o.id,
                        "title": o.title,
                        "description": o.description[:200] if o.description else None,
                        "category": o.category,
                        "score": o.feasibility_score,
                        "created_at": o.created_at.isoformat() if o.created_at else None,
                    }
                    for o in opportunities[:20]
                ],
                "trends": [
                    {
                        "id": t.id,
                        "name": t.trend_name,
                        "strength": t.trend_strength,
                        "description": t.description,
                        "growth_rate": t.growth_rate,
                        "opportunities_count": t.opportunities_count,
                    }
                    for t in trends
                ],
                "synthesis": synthesis,
                "total_count": len(opportunities),
                "processing_time_ms": processing_time,
            }
            
            await self._log_activity(
                user_id=user_id,
                session_id=session_id,
                path=ConsultantPath.search_ideas.value,
                action="search_complete",
                payload=filters,
                result_summary=f"Found {len(opportunities)} opportunities, {len(trends)} trends",
                ai_model_used="hybrid",
                processing_time_ms=processing_time,
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error searching ideas: {e}")
            return {"success": False, "error": str(e)}

    async def identify_location(
        self,
        user_id: int,
        city: str,
        business_description: str,
        additional_params: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Path 3: Identify Location - Geographic intelligence
        Accepts natural language business description and infers category automatically.
        """
        import time
        start_time = time.time()
        
        try:
            inferred_category = self._infer_business_category(business_description)
            logger.info(f"[TIMING] Category inference: {int((time.time() - start_time) * 1000)}ms")
            
            cache_key = self._generate_cache_key(city, business_description, None, additional_params)
            
            cached = self._get_cached_analysis(cache_key)
            if cached:
                cached["from_cache"] = True
                cached["cache_hit_count"] = cached.get("hit_count", 1)
                return cached
            
            geo_start = time.time()
            geo_analysis = await self._deepseek_geo_analysis(
                city, business_description, inferred_category, additional_params
            )
            logger.info(f"[TIMING] Geo analysis: {int((time.time() - geo_start) * 1000)}ms")
            
            market_report = self._generate_quick_market_report(
                city, business_description, inferred_category, geo_analysis
            )
            logger.info(f"[TIMING] Quick market report generated")
            
            site_recommendations = self._generate_site_recommendations(
                geo_analysis, market_report, inferred_category
            )
            
            processing_time = int((time.time() - start_time) * 1000)
            
            competitors = geo_analysis.get("competitors", [])
            pins = []
            for idx, comp in enumerate(competitors):
                if comp.get("lat") and comp.get("lng"):
                    pins.append({
                        "id": idx + 1,
                        "lat": comp.get("lat"),
                        "lng": comp.get("lng"),
                        "name": comp.get("name", "Unknown"),
                        "rating": comp.get("rating"),
                        "reviews": comp.get("reviews"),
                        "source": "google_maps",
                        "popup": comp.get("address", ""),
                    })
            
            if pins:
                center_lat = pins[0]["lat"]
                center_lng = pins[0]["lng"]
            else:
                parsed_city, parsed_state = parse_city_state(city)
                fallback_coords = self._get_city_center_coords(parsed_city, parsed_state)
                center_lat = fallback_coords["lat"]
                center_lng = fallback_coords["lng"]
            
            map_data = {
                "city": city,
                "center": {"lat": center_lat, "lng": center_lng},
                "layers": {
                    "pins": {"type": "pins", "data": pins, "count": len(pins)},
                    "heatmap": {"type": "heatmap", "data": [], "count": 0},
                    "polygons": {"type": "polygons", "data": [], "count": 0},
                },
                "totalFeatures": len(pins),
            }
            
            result = {
                "success": True,
                "city": city,
                "business_description": business_description,
                "inferred_category": inferred_category,
                "geo_analysis": geo_analysis,
                "market_report": market_report,
                "site_recommendations": site_recommendations,
                "map_data": map_data,
                "from_cache": False,
                "processing_time_ms": processing_time,
            }
            
            await self._cache_analysis(
                cache_key=cache_key,
                city=city,
                business_type=inferred_category,
                business_subtype=business_description,
                query_params=additional_params,
                geo_analysis=geo_analysis,
                market_report=market_report,
                site_recommendations=site_recommendations,
            )
            
            await self._log_activity(
                user_id=user_id,
                session_id=session_id,
                path=ConsultantPath.identify_location.value,
                action="location_analysis_complete",
                payload={"city": city, "business_description": business_description, "category": inferred_category},
                result_summary=f"Analysis for {business_description} in {city}",
                ai_model_used="hybrid",
                processing_time_ms=processing_time,
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error analyzing location: {e}")
            return {"success": False, "error": str(e)}

    async def clone_success(
        self,
        user_id: int,
        business_name: str,
        business_address: str,
        target_city: Optional[str] = None,
        target_state: Optional[str] = None,
        radius_miles: int = 3,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Path 4: Clone Success - Replicate successful business models
        Analyzes a successful business and finds similar markets to replicate it.
        Now searches within a specific target city/state for matching locations.
        """
        import time
        start_time = time.time()
        
        try:
            source_analysis = await self._analyze_source_business(
                business_name, business_address, radius_miles
            )
            
            matching_locations = await self._find_matching_locations(
                source_analysis, radius_miles, target_city, target_state
            )
            
            processing_time = int((time.time() - start_time) * 1000)
            
            result = {
                "success": True,
                "source_business": source_analysis,
                "matching_locations": matching_locations,
                "analysis_radius_miles": radius_miles,
                "processing_time_ms": processing_time,
            }
            
            await self._log_activity(
                user_id=user_id,
                session_id=session_id,
                path="clone_success",
                action="clone_analysis_complete",
                payload={
                    "business_name": business_name,
                    "address": business_address,
                    "radius": radius_miles,
                },
                result_summary=f"Found {len(matching_locations)} matching locations for {business_name}",
                ai_model_used="hybrid",
                processing_time_ms=processing_time,
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error in clone success analysis: {e}")
            return {"success": False, "error": str(e), "analysis_radius_miles": radius_miles}

    async def deep_clone_analysis(
        self,
        user_id: int,
        source_business_name: str,
        source_business_address: str,
        target_city: str,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Premium: Deep Clone Analysis - Detailed 3mi and 5mi radius analysis for a specific target city.
        """
        import time
        start_time = time.time()
        
        try:
            import asyncio
            
            source_analysis = await self._analyze_source_business(
                source_business_name, source_business_address, 3
            )
            
            three_mile, five_mile = await asyncio.gather(
                self._analyze_target_city_radius(
                    target_city, source_analysis.get("category", "retail"), 3
                ),
                self._analyze_target_city_radius(
                    target_city, source_analysis.get("category", "retail"), 5
                )
            )
            
            three_mile_quality = three_mile.get("data_quality", "high")
            five_mile_quality = five_mile.get("data_quality", "high")
            overall_data_quality = "high"
            if three_mile_quality == "limited" or five_mile_quality == "limited":
                overall_data_quality = "limited"
            elif three_mile_quality == "estimated" or five_mile_quality == "estimated":
                overall_data_quality = "estimated"
            
            match_score = self._calculate_match_score(
                source_analysis, three_mile, five_mile, overall_data_quality
            )
            key_factors = self._extract_key_factors(source_analysis, three_mile, five_mile)
            
            processing_time = int((time.time() - start_time) * 1000)
            
            result = {
                "success": True,
                "source_business": source_analysis,
                "target_city": target_city,
                "three_mile_analysis": three_mile,
                "five_mile_analysis": five_mile,
                "match_score": match_score,
                "key_factors": key_factors,
                "data_quality": overall_data_quality,
                "processing_time_ms": processing_time,
                "requires_payment": False,
            }
            
            await self._log_activity(
                user_id=user_id,
                session_id=session_id,
                path="deep_clone",
                action="deep_clone_complete",
                payload={
                    "source_business": source_business_name,
                    "target_city": target_city,
                },
                result_summary=f"Deep analysis of {target_city} for {source_business_name} - {match_score}% match",
                ai_model_used="hybrid",
                processing_time_ms=processing_time,
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error in deep clone analysis: {e}")
            return {"success": False, "error": str(e)}

    async def _analyze_target_city_radius(
        self,
        target_city: str,
        category: str,
        radius_miles: int,
    ) -> Dict[str, Any]:
        """Analyze a target city at a specific radius using real data sources.
        
        Uses:
        - Census Bureau API for demographics (state-level with estimation notes)
        - SerpAPI Google Maps for competition analysis with radius filtering
        """
        import os
        import httpx
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        parsed_city, parsed_state = self._parse_address_components(target_city)
        if not parsed_state:
            parts = target_city.split(",")
            if len(parts) >= 2:
                parsed_state = parts[-1].strip().upper()[:2]
        
        coordinates = await self._geocode_city(target_city)
        
        demographics = await self._fetch_city_demographics(parsed_state)
        is_demographics_estimated = demographics.get("is_estimated", False)
        
        competition_data = await self._fetch_competition_data(
            category, target_city, radius_miles, coordinates
        )
        
        population = demographics.get("population", 0)
        median_income = demographics.get("median_income", 0)
        median_age = demographics.get("median_age", 0)
        households = demographics.get("total_households", 0)
        competition_count = competition_data.get("count", 0)
        
        market_density = "high" if competition_count > 8 else "medium" if competition_count > 3 else "low"
        competition_level = "high" if competition_count > 10 else "medium" if competition_count > 5 else "low"
        
        growth_rate = demographics.get("growth_rate", 3.0)
        
        demographics_quality = "estimated" if is_demographics_estimated else "state_level"
        competition_quality = competition_data.get("competition_quality", "limited")
        
        overall_quality = "estimated"
        if demographics_quality == "estimated" or competition_quality == "limited":
            overall_quality = "limited"
        
        data_scope_warning = None
        if demographics.get("data_level") == "state":
            data_scope_warning = "Demographics are state-level averages. Competition data is filtered to the specified radius."
        
        return {
            "radius_miles": radius_miles,
            "state_demographics": {
                "population": population,
                "median_income": median_income,
                "median_age": median_age,
                "households": households,
                "unemployment_rate": demographics.get("unemployment_rate", 0),
                "median_home_value": demographics.get("median_home_value", 0),
                "data_level": demographics.get("data_level", "state"),
                "scope": "state",
            },
            "competition": {
                "count": competition_count,
                "competitors": competition_data.get("competitors", [])[:5],
                "total_found": competition_data.get("total_found", 0),
                "geocoded_ratio": competition_data.get("geocoded_ratio", 0),
                "scope": competition_data.get("competition_scope", "area"),
            },
            "competition_count": competition_count,
            "market_density": market_density,
            "competition_level": competition_level,
            "growth_rate": round(growth_rate, 1),
            "data_source": "US Census Bureau ACS + Google Maps",
            "data_quality": overall_quality,
            "demographics_quality": demographics_quality,
            "demographics_scope": "state",
            "competition_quality": competition_quality,
            "competition_scope": competition_data.get("competition_scope", "area"),
            "radius_filtered": competition_data.get("radius_filtered", False),
            "data_scope_warning": data_scope_warning,
            "coordinates": coordinates,
        }
    
    async def _geocode_city(self, city: str) -> Optional[Dict[str, float]]:
        """Get coordinates for a city using SerpAPI."""
        import os
        
        serpapi_key = os.environ.get("SERPAPI_KEY")
        if not serpapi_key:
            return None
        
        try:
            from .serpapi_service import SerpAPIService
            serpapi = SerpAPIService()
            
            if not serpapi.is_configured:
                return None
            
            import asyncio
            from concurrent.futures import ThreadPoolExecutor
            
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    lambda: serpapi.google_maps_search(
                        query=city,
                        type="search"
                    )
                )
            
            place_results = result.get("place_results", {})
            if place_results:
                gps = place_results.get("gps_coordinates", {})
                if gps:
                    return {"lat": gps.get("latitude"), "lng": gps.get("longitude")}
            
            local_results = result.get("local_results", [])
            if local_results and len(local_results) > 0:
                gps = local_results[0].get("gps_coordinates", {})
                if gps:
                    return {"lat": gps.get("latitude"), "lng": gps.get("longitude")}
            
            return None
            
        except Exception as e:
            logger.warning(f"Geocoding failed for {city}: {e}")
            return None
    
    async def _fetch_city_demographics(self, state: str) -> Dict[str, Any]:
        """Fetch demographics from Census Bureau API for a state.
        
        Note: Returns state-level data with is_estimated flag to indicate
        these are regional averages, not city-specific data.
        """
        import os
        import httpx
        
        api_key = os.environ.get("CENSUS_API_KEY")
        if not api_key or not state:
            logger.warning("Census API key or state not available, using fallback")
            fallback = self._get_fallback_demographics()
            fallback["is_estimated"] = True
            return fallback
        
        state_fips = self._get_state_fips(state)
        if not state_fips:
            logger.warning(f"Unknown state: {state}, using fallback")
            fallback = self._get_fallback_demographics()
            fallback["is_estimated"] = True
            return fallback
        
        try:
            variables = [
                "B01003_001E",  # Total population
                "B01002_001E",  # Median age
                "B19013_001E",  # Median household income
                "B11001_001E",  # Total households
                "B25077_001E",  # Median home value
                "B23025_005E",  # Unemployed
                "B23025_002E",  # Labor force
            ]
            
            url = f"https://api.census.gov/data/2023/acs/acs5?get={','.join(variables)}&for=state:{state_fips}&key={api_key}"
            
            async with httpx.AsyncClient(timeout=15.0) as client:
                response = await client.get(url)
                response.raise_for_status()
                data = response.json()
            
            if len(data) < 2:
                fallback = self._get_fallback_demographics()
                fallback["is_estimated"] = True
                return fallback
            
            values = data[1]
            
            def safe_int(val):
                try:
                    return int(val) if val and val != '-' else 0
                except:
                    return 0
            
            population = safe_int(values[0])
            median_age = safe_int(values[1])
            median_income = safe_int(values[2])
            households = safe_int(values[3])
            home_value = safe_int(values[4])
            unemployed = safe_int(values[5])
            labor_force = safe_int(values[6])
            
            unemployment_rate = (unemployed / labor_force * 100) if labor_force > 0 else 0
            
            return {
                "population": population,
                "median_age": median_age,
                "median_income": median_income,
                "total_households": households,
                "median_home_value": home_value,
                "unemployment_rate": round(unemployment_rate, 1),
                "growth_rate": 3.5,
                "is_estimated": False,
                "data_level": "state",
            }
            
        except Exception as e:
            logger.error(f"Census API error: {e}")
            fallback = self._get_fallback_demographics()
            fallback["is_estimated"] = True
            return fallback
    
    async def _fetch_competition_data(
        self, category: str, location: str, radius_miles: int,
        coordinates: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """Fetch competition data using SerpAPI Google Maps with radius filtering."""
        import os
        
        serpapi_key = os.environ.get("SERPAPI_KEY")
        if not serpapi_key:
            logger.warning("SERPAPI_KEY not configured, using fallback")
            return {"count": 0, "competitors": [], "radius_filtered": False}
        
        try:
            from .serpapi_service import SerpAPIService
            serpapi = SerpAPIService()
            
            if not serpapi.is_configured:
                return {"count": 0, "competitors": [], "radius_filtered": False}
            
            search_query = f"{category} near {location}"
            
            ll_param = None
            if coordinates and coordinates.get("lat") and coordinates.get("lng"):
                lat = coordinates["lat"]
                lng = coordinates["lng"]
                zoom = 14 if radius_miles <= 3 else 12 if radius_miles <= 5 else 10
                ll_param = f"@{lat},{lng},{zoom}z"
            
            import asyncio
            from concurrent.futures import ThreadPoolExecutor
            
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    lambda: serpapi.google_maps_search(
                        query=search_query,
                        location=location,
                        ll=ll_param
                    )
                )
            
            local_results = result.get("local_results", [])
            
            radius_km = radius_miles * 1.60934
            filtered_results = local_results
            
            if coordinates and coordinates.get("lat") and coordinates.get("lng"):
                center_lat = coordinates["lat"]
                center_lng = coordinates["lng"]
                filtered_results = []
                
                for place in local_results:
                    place_gps = place.get("gps_coordinates", {})
                    if place_gps:
                        place_lat = place_gps.get("latitude", 0)
                        place_lng = place_gps.get("longitude", 0)
                        
                        distance = self._haversine_distance(
                            center_lat, center_lng, place_lat, place_lng
                        )
                        
                        if distance <= radius_km:
                            place["distance_km"] = round(distance, 2)
                            filtered_results.append(place)
                    else:
                        filtered_results.append(place)
            
            geocoded_count = sum(1 for p in filtered_results if p.get("distance_km") is not None)
            total_results = len(local_results)
            geocoded_ratio = geocoded_count / total_results if total_results > 0 else 0
            
            competitors = []
            for place in filtered_results[:10]:
                if place.get("distance_km") is not None or not coordinates:
                    competitors.append({
                        "name": place.get("title", "Unknown"),
                        "rating": place.get("rating", 0),
                        "reviews": place.get("reviews", 0),
                        "address": place.get("address", ""),
                        "distance_km": place.get("distance_km"),
                    })
            
            if not coordinates:
                competition_quality = "limited"
            elif geocoded_ratio >= 0.7:
                competition_quality = "high"
            elif geocoded_ratio >= 0.3:
                competition_quality = "estimated"
            else:
                competition_quality = "limited"
            
            return {
                "count": len([c for c in competitors if c.get("distance_km") is not None]) if coordinates else len(competitors),
                "total_found": total_results,
                "geocoded_ratio": round(geocoded_ratio, 2),
                "competitors": competitors,
                "radius_filtered": coordinates is not None,
                "competition_quality": competition_quality,
                "competition_scope": "radius" if coordinates else "area",
            }
            
        except Exception as e:
            logger.error(f"SerpAPI competition search error: {e}")
            return {"count": 0, "competitors": [], "radius_filtered": False}
    
    def _haversine_distance(
        self, lat1: float, lon1: float, lat2: float, lon2: float
    ) -> float:
        """Calculate the great circle distance between two points in km."""
        import math
        
        R = 6371
        
        lat1_rad = math.radians(lat1)
        lat2_rad = math.radians(lat2)
        delta_lat = math.radians(lat2 - lat1)
        delta_lon = math.radians(lon2 - lon1)
        
        a = (math.sin(delta_lat / 2) ** 2 +
             math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2) ** 2)
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        
        return R * c
    
    def _get_fallback_demographics(self) -> Dict[str, Any]:
        """Return fallback demographics when API is unavailable."""
        import random
        return {
            "population": random.randint(100000, 500000),
            "median_age": random.randint(32, 42),
            "median_income": random.randint(55000, 85000),
            "total_households": random.randint(40000, 200000),
            "median_home_value": random.randint(200000, 500000),
            "unemployment_rate": round(random.uniform(3.0, 6.0), 1),
            "growth_rate": round(random.uniform(2.0, 5.0), 1),
        }
    
    def _get_state_fips(self, state: str) -> Optional[str]:
        """Convert state abbreviation to FIPS code."""
        fips_map = {
            'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06',
            'CO': '08', 'CT': '09', 'DE': '10', 'FL': '12', 'GA': '13',
            'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19',
            'KS': '20', 'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24',
            'MA': '25', 'MI': '26', 'MN': '27', 'MS': '28', 'MO': '29',
            'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34',
            'NM': '35', 'NY': '36', 'NC': '37', 'ND': '38', 'OH': '39',
            'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44', 'SC': '45',
            'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50',
            'VA': '51', 'WA': '53', 'WV': '54', 'WI': '55', 'WY': '56',
            'DC': '11', 'PR': '72',
        }
        return fips_map.get(state.upper() if state else "")

    def _calculate_match_score(
        self,
        source: Dict[str, Any],
        three_mile: Dict[str, Any],
        five_mile: Dict[str, Any],
        data_quality: str = "high",
    ) -> int:
        """Calculate overall match score between source and target using real metrics.
        
        Scoring factors:
        - Income similarity (20 points max)
        - Competition opportunity (25 points max)
        - Market density (20 points max)
        - Demographics match (20 points max)
        - Growth potential (15 points max)
        
        Score is capped based on data quality:
        - high: up to 100
        - estimated: up to 85
        - limited: up to 70
        """
        score = 50
        
        source_income = source.get("demographics", {}).get("median_income", 0) or source.get("median_income", 60000)
        target_demographics = three_mile.get("state_demographics", {})
        target_income = target_demographics.get("median_income", 0) or three_mile.get("median_income", 0)
        
        if target_income > 0 and source_income > 0:
            income_ratio = min(target_income, source_income) / max(target_income, source_income)
            score += int(income_ratio * 20)
        
        competition_level = three_mile.get("competition_level", "medium")
        if competition_level == "low":
            score += 25
        elif competition_level == "medium":
            score += 15
        else:
            score += 5
        
        market_density = three_mile.get("market_density", "medium")
        if market_density == "high":
            score += 20
        elif market_density == "medium":
            score += 12
        else:
            score += 5
        
        source_age = source.get("demographics", {}).get("median_age", 0) or source.get("median_age", 35)
        target_age = target_demographics.get("median_age", 0) or three_mile.get("median_age", 0)
        
        if target_age > 0 and source_age > 0:
            age_diff = abs(target_age - source_age)
            if age_diff <= 5:
                score += 20
            elif age_diff <= 10:
                score += 12
            else:
                score += 5
        
        growth_rate = three_mile.get("growth_rate", 3.0)
        if growth_rate > 5:
            score += 15
        elif growth_rate > 3:
            score += 10
        else:
            score += 5
        
        max_score = 100
        if data_quality == "limited":
            max_score = 70
        elif data_quality == "estimated":
            max_score = 85
        
        return min(max_score, max(0, score))

    def _extract_key_factors(
        self,
        source: Dict[str, Any],
        three_mile: Dict[str, Any],
        five_mile: Dict[str, Any],
    ) -> List[str]:
        """Extract key matching factors using real data comparisons."""
        factors = []
        
        competition_count = three_mile.get("competition_count", 0)
        competition_level = three_mile.get("competition_level", "medium")
        
        if competition_level == "low":
            factors.append(f"Low competition ({competition_count} competitors in 3mi radius)")
        elif competition_level == "medium":
            factors.append(f"Moderate competition ({competition_count} competitors) - room to differentiate")
        else:
            factors.append(f"Competitive market ({competition_count} competitors) - requires strong positioning")
        
        three_demographics = three_mile.get("state_demographics", {})
        five_demographics = five_mile.get("state_demographics", {})
        
        median_income = three_demographics.get("median_income", 0) or three_mile.get("median_income", 0)
        if median_income > 80000:
            factors.append(f"High-income region (${median_income:,} state median)")
        elif median_income > 60000:
            factors.append(f"Above-average income region (${median_income:,} state median)")
        elif median_income > 0:
            factors.append(f"Value-conscious region (${median_income:,} state median)")
        
        market_density = three_mile.get("market_density", "medium")
        if market_density == "high":
            factors.append("Strong local market density based on competition")
        elif market_density == "medium":
            factors.append("Moderate market activity in the area")
        
        growth_rate = three_mile.get("growth_rate", 0)
        if growth_rate > 5:
            factors.append(f"High growth market ({growth_rate}% annual growth)")
        elif growth_rate > 3:
            factors.append(f"Steady growth trajectory ({growth_rate}% growth)")
        
        five_competition = five_mile.get("competition_count", 0)
        if five_competition > competition_count:
            factors.append(f"Expanding market in 5mi radius ({five_competition} competitors)")
        
        unemployment = three_demographics.get("unemployment_rate", 0)
        if unemployment > 0 and unemployment < 4:
            factors.append(f"Strong regional economy ({unemployment}% unemployment)")
        
        if not factors:
            factors = ["Emerging market opportunity", "Strategic location for new entry"]
        
        return factors[:6]

    async def _analyze_source_business(
        self,
        business_name: str,
        business_address: str,
        radius_miles: int,
    ) -> Dict[str, Any]:
        """Analyze the source business to extract success factors"""
        from .trade_area_analyzer import trade_area_analyzer
        
        inferred_category = self._infer_business_category(business_name)
        
        parsed_city, parsed_state = self._parse_address_components(business_address)
        
        try:
            opportunity_data = {
                "id": 0,
                "title": business_name,
                "business_description": f"{business_name} in {business_address}",
                "category": inferred_category,
                "location": business_address,
                "city": parsed_city,
                "state": parsed_state,
            }
            
            trade_area = await trade_area_analyzer.analyze_async(
                opportunity_data,
                include_ai_synthesis=False
            )
            demographics = trade_area.demographics or {}
            competitors = trade_area.competitors or []
            
            success_factors = []
            if demographics.get("median_income"):
                income = demographics.get("median_income", 0)
                if income > 75000:
                    success_factors.append("High income area ($75K+)")
                elif income > 50000:
                    success_factors.append("Middle income area ($50K-$75K)")
            
            if demographics.get("median_age"):
                age = demographics.get("median_age", 0)
                if 25 <= age <= 40:
                    success_factors.append("Young professional demographic")
                elif 35 <= age <= 55:
                    success_factors.append("Established family demographic")
            
            if len(competitors) < 5:
                success_factors.append("Low competition environment")
            elif len(competitors) > 10:
                success_factors.append("Proven market demand")
            
            if trade_area.white_space_score > 60:
                success_factors.append("High white space opportunity")
            
            if demographics.get("population"):
                pop = demographics.get("population", 0)
                if pop > 100000:
                    success_factors.append("Dense population center")
            
            return {
                "name": business_name,
                "address": business_address,
                "category": inferred_category,
                "success_factors": success_factors if success_factors else ["Market presence", "Location accessibility"],
                "demographics": {
                    "population": demographics.get("population", "N/A"),
                    "median_income": demographics.get("median_income", "N/A"),
                    "median_age": demographics.get("median_age", "N/A"),
                },
                "competition_count": len(competitors),
                "trade_area_radius_miles": radius_miles,
            }
            
        except Exception as e:
            logger.warning(f"Source business analysis failed: {e}")
            return {
                "name": business_name,
                "address": business_address,
                "category": inferred_category,
                "success_factors": ["Market presence", "Strategic location"],
                "demographics": {},
            }

    async def _find_matching_locations(
        self,
        source_analysis: Dict[str, Any],
        radius_miles: int,
        target_city: Optional[str] = None,
        target_state: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Find locations with similar demographics and market conditions.
        If target_city/state provided, searches for neighborhoods within that area.
        Returns locations with lat/lng coordinates for map display.
        """
        import random
        
        target_demographics = source_analysis.get("demographics", {})
        category = source_analysis.get("category", "retail")
        
        def safe_numeric(value, default):
            """Convert value to numeric, handling 'N/A' and other non-numeric strings"""
            if value is None or value == "N/A" or value == "":
                return default
            try:
                return float(value) if isinstance(value, str) else value
            except (ValueError, TypeError):
                return default
        
        source_income = safe_numeric(target_demographics.get("median_income"), 65000)
        source_pop = safe_numeric(target_demographics.get("population"), 100000)
        
        if target_city and target_state:
            locations = await self._get_neighborhoods_in_city(target_city, target_state, category)
        else:
            locations = self._get_default_metro_areas()
        
        matching_locations = []
        for loc in locations:
            variance = random.randint(-8, 12)
            base_score = loc.get("base_score", 70)
            similarity_score = min(100, max(45, base_score + variance))
            
            pop_variance = random.uniform(0.7, 1.4)
            income_variance = random.uniform(0.8, 1.3)
            estimated_pop = int(source_pop * pop_variance * (1 + random.uniform(-0.2, 0.3)))
            estimated_income = int(source_income * income_variance)
            competition_count = random.randint(1, 12)
            
            demographics_match = min(100, max(35, similarity_score + random.randint(-12, 8)))
            competition_match = min(100, max(35, similarity_score + random.randint(-15, 5)))
            
            key_factors = []
            if demographics_match > 75:
                key_factors.append("Similar income levels")
            if competition_match > 70:
                key_factors.append("Low competition area")
            if similarity_score > 80:
                key_factors.append("High growth market")
            if estimated_pop > 80000:
                key_factors.append("Dense population center")
            if category in ["hospitality", "retail", "food_service"]:
                key_factors.append("Strong foot traffic potential")
            if competition_count < 5:
                key_factors.append("Underserved market")
            
            matching_locations.append({
                "name": loc["name"],
                "city": loc["city"],
                "state": loc["state"],
                "lat": loc["lat"],
                "lng": loc["lng"],
                "address": loc.get("address", ""),
                "similarity_score": similarity_score,
                "demographics_match": demographics_match,
                "competition_match": competition_match,
                "population": estimated_pop,
                "median_income": estimated_income,
                "competition_count": competition_count,
                "key_factors": key_factors if key_factors else ["Growing market", "Business-friendly environment"],
            })
        
        matching_locations.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        return matching_locations[:3]
    
    async def _get_neighborhoods_in_city(
        self,
        city: str,
        state: str,
        category: str,
    ) -> List[Dict[str, Any]]:
        """Get real neighborhood locations within a specific city using SerpAPI for accurate coordinates"""
        import random
        import asyncio
        from .serpapi_service import serpapi_service
        
        neighborhoods = []
        
        search_terms = [
            f"shopping plaza {city} {state}",
            f"business center {city} {state}",
            f"commercial district {city} {state}",
        ]
        
        if serpapi_service.is_configured:
            try:
                from .location_utils import validate_coordinates_in_state, normalize_state
                
                async def fetch_serpapi(search_term: str):
                    return await asyncio.to_thread(
                        serpapi_service.google_maps_search,
                        query=search_term,
                        location=f"{city}, {state}"
                    )
                
                results = await asyncio.gather(
                    *[fetch_serpapi(term) for term in search_terms],
                    return_exceptions=True
                )
                
                normalized_state = normalize_state(state) or state.upper()
                
                for result in results:
                    if isinstance(result, (Exception, BaseException)):
                        continue
                    if not isinstance(result, dict):
                        continue
                    
                    local_results = result.get("local_results", [])
                    
                    for place in local_results[:3]:
                        if len(neighborhoods) >= 5:
                            break
                        gps = place.get("gps_coordinates", {})
                        if gps and "latitude" in gps and "longitude" in gps:
                            lat = gps["latitude"]
                            lng = gps["longitude"]
                            
                            is_valid, warning = validate_coordinates_in_state(
                                lat, lng, state,
                                context=f"SerpAPI result for '{city}, {state}' - {place.get('title', 'unknown')}"
                            )
                            if not is_valid:
                                logger.warning(f"Skipping location with mismatched coordinates: {warning}")
                                continue
                            
                            address = place.get("address", "")
                            name = place.get("title", f"Location in {city}")
                            
                            if len(name) > 40:
                                name = name[:37] + "..."
                            
                            neighborhoods.append({
                                "name": name,
                                "city": city.title(),
                                "state": normalized_state,
                                "lat": round(lat, 6),
                                "lng": round(lng, 6),
                                "address": address,
                                "base_score": random.randint(65, 92),
                            })
                    
                    if len(neighborhoods) >= 5:
                        break
                
                if neighborhoods:
                    logger.info(f"Found {len(neighborhoods)} real locations in {city}, {state}")
                    return neighborhoods
                    
            except Exception as e:
                logger.warning(f"SerpAPI search failed for {city}, {state}: {e}")
        
        known_locations = {
            ("miami", "fl"): [
                {"name": "Brickell City Centre", "lat": 25.7650, "lng": -80.1936, "address": "701 S Miami Ave, Miami, FL"},
                {"name": "Dadeland Mall", "lat": 25.6903, "lng": -80.3140, "address": "7535 N Kendall Dr, Miami, FL"},
                {"name": "Aventura Mall", "lat": 25.9569, "lng": -80.1414, "address": "19501 Biscayne Blvd, Aventura, FL"},
                {"name": "Dolphin Mall", "lat": 25.7883, "lng": -80.3827, "address": "11401 NW 12th St, Miami, FL"},
                {"name": "Coral Gables Downtown", "lat": 25.7496, "lng": -80.2619, "address": "355 Miracle Mile, Coral Gables, FL"},
            ],
            ("orlando", "fl"): [
                {"name": "The Mall at Millenia", "lat": 28.4848, "lng": -81.4314, "address": "4200 Conroy Rd, Orlando, FL"},
                {"name": "Orlando Fashion Square", "lat": 28.5529, "lng": -81.3407, "address": "3201 E Colonial Dr, Orlando, FL"},
                {"name": "Winter Park Village", "lat": 28.5970, "lng": -81.3510, "address": "510 N Orlando Ave, Winter Park, FL"},
            ],
            ("austin", "tx"): [
                {"name": "The Domain", "lat": 30.4020, "lng": -97.7254, "address": "11410 Century Oaks Terrace, Austin, TX"},
                {"name": "Barton Creek Square", "lat": 30.2610, "lng": -97.8082, "address": "2901 Capital of Texas Hwy, Austin, TX"},
                {"name": "Downtown Austin", "lat": 30.2672, "lng": -97.7431, "address": "Congress Ave, Austin, TX"},
            ],
            ("dallas", "tx"): [
                {"name": "NorthPark Center", "lat": 32.8680, "lng": -96.7728, "address": "8687 N Central Expy, Dallas, TX"},
                {"name": "Galleria Dallas", "lat": 32.9308, "lng": -96.8198, "address": "13350 Dallas Pkwy, Dallas, TX"},
                {"name": "Highland Park Village", "lat": 32.8362, "lng": -96.7993, "address": "47 Highland Park Village, Dallas, TX"},
            ],
            ("denver", "co"): [
                {"name": "Cherry Creek Shopping Center", "lat": 39.7157, "lng": -104.9536, "address": "3000 E 1st Ave, Denver, CO"},
                {"name": "Park Meadows", "lat": 39.5634, "lng": -104.8791, "address": "8401 Park Meadows Center Dr, Lone Tree, CO"},
                {"name": "16th Street Mall", "lat": 39.7476, "lng": -104.9940, "address": "16th Street Mall, Denver, CO"},
            ],
            ("atlanta", "ga"): [
                {"name": "Lenox Square", "lat": 33.8463, "lng": -84.3608, "address": "3393 Peachtree Rd NE, Atlanta, GA"},
                {"name": "Phipps Plaza", "lat": 33.8500, "lng": -84.3611, "address": "3500 Peachtree Rd NE, Atlanta, GA"},
                {"name": "Atlantic Station", "lat": 33.7910, "lng": -84.3960, "address": "1380 Atlantic Dr NW, Atlanta, GA"},
            ],
        }
        
        city_key = (city.lower().strip(), state.lower().strip())
        
        if city_key in known_locations:
            for loc in known_locations[city_key][:5]:
                neighborhoods.append({
                    "name": loc["name"],
                    "city": city.title(),
                    "state": state.upper(),
                    "lat": loc["lat"],
                    "lng": loc["lng"],
                    "address": loc.get("address", ""),
                    "base_score": random.randint(65, 90),
                })
            return neighborhoods
        
        logger.warning(f"No real location data available for {city}, {state}. Using city center.")
        city_centers = {
            ("west palm beach", "fl"): {"lat": 26.7153, "lng": -80.0534},
            ("fort walton beach", "fl"): {"lat": 30.4057, "lng": -86.6189},
            ("destin", "fl"): {"lat": 30.3935, "lng": -86.4958},
            ("panama city", "fl"): {"lat": 30.1588, "lng": -85.6602},
            ("pensacola", "fl"): {"lat": 30.4213, "lng": -87.2169},
            ("tallahassee", "fl"): {"lat": 30.4383, "lng": -84.2807},
            ("tampa", "fl"): {"lat": 27.9506, "lng": -82.4572},
            ("jacksonville", "fl"): {"lat": 30.3322, "lng": -81.6557},
            ("sarasota", "fl"): {"lat": 27.3364, "lng": -82.5307},
            ("naples", "fl"): {"lat": 26.1420, "lng": -81.7948},
            ("houston", "tx"): {"lat": 29.7604, "lng": -95.3698},
            ("san antonio", "tx"): {"lat": 29.4241, "lng": -98.4936},
            ("fort worth", "tx"): {"lat": 32.7555, "lng": -97.3308},
            ("el paso", "tx"): {"lat": 31.7619, "lng": -106.4850},
            ("phoenix", "az"): {"lat": 33.4484, "lng": -112.0740},
            ("scottsdale", "az"): {"lat": 33.4942, "lng": -111.9261},
            ("tucson", "az"): {"lat": 32.2226, "lng": -110.9747},
            ("charlotte", "nc"): {"lat": 35.2271, "lng": -80.8431},
            ("raleigh", "nc"): {"lat": 35.7796, "lng": -78.6382},
            ("nashville", "tn"): {"lat": 36.1627, "lng": -86.7816},
            ("memphis", "tn"): {"lat": 35.1495, "lng": -90.0490},
            ("los angeles", "ca"): {"lat": 34.0522, "lng": -118.2437},
            ("san diego", "ca"): {"lat": 32.7157, "lng": -117.1611},
            ("san francisco", "ca"): {"lat": 37.7749, "lng": -122.4194},
            ("seattle", "wa"): {"lat": 47.6062, "lng": -122.3321},
            ("new york", "ny"): {"lat": 40.7128, "lng": -74.0060},
            ("chicago", "il"): {"lat": 41.8781, "lng": -87.6298},
            ("boston", "ma"): {"lat": 42.3601, "lng": -71.0589},
            ("las vegas", "nv"): {"lat": 36.1699, "lng": -115.1398},
            ("portland", "or"): {"lat": 45.5152, "lng": -122.6784},
            ("salt lake city", "ut"): {"lat": 40.7608, "lng": -111.8910},
        }
        
        base = city_centers.get(city_key)
        
        if not base and serpapi_service.is_configured:
            try:
                geocode_result = serpapi_service.google_maps_search(
                    query=f"{city}, {state}",
                    location=f"{city}, {state}"
                )
                local_results = geocode_result.get("local_results", [])
                if local_results:
                    gps = local_results[0].get("gps_coordinates", {})
                    if gps and "latitude" in gps and "longitude" in gps:
                        base = {"lat": gps["latitude"], "lng": gps["longitude"]}
                        logger.info(f"Geocoded {city}, {state} via SerpAPI: {base}")
            except Exception as e:
                logger.warning(f"SerpAPI geocoding failed for {city}, {state}: {e}")
        
        if not base:
            base = {"lat": 33.0, "lng": -97.0}
        
        neighborhoods.append({
            "name": f"Downtown {city.title()}",
            "city": city.title(),
            "state": state.upper(),
            "lat": base["lat"],
            "lng": base["lng"],
            "address": f"City Center, {city.title()}, {state.upper()}",
            "base_score": 75,
        })
        
        return neighborhoods
    
    def _get_default_metro_areas(self) -> List[Dict[str, Any]]:
        """Get default list of metro areas when no target city is specified"""
        return [
            {"name": "Downtown Austin", "city": "Austin", "state": "TX", "lat": 30.2672, "lng": -97.7431, "base_score": 88},
            {"name": "South Denver", "city": "Denver", "state": "CO", "lat": 39.6501, "lng": -104.9903, "base_score": 85},
            {"name": "East Nashville", "city": "Nashville", "state": "TN", "lat": 36.1800, "lng": -86.7500, "base_score": 82},
            {"name": "Uptown Charlotte", "city": "Charlotte", "state": "NC", "lat": 35.2271, "lng": -80.8431, "base_score": 80},
            {"name": "North Phoenix", "city": "Phoenix", "state": "AZ", "lat": 33.5800, "lng": -112.0740, "base_score": 78},
            {"name": "Downtown Raleigh", "city": "Raleigh", "state": "NC", "lat": 35.7796, "lng": -78.6382, "base_score": 77},
            {"name": "Sugar House", "city": "Salt Lake City", "state": "UT", "lat": 40.7233, "lng": -111.8575, "base_score": 75},
            {"name": "Hyde Park", "city": "Tampa", "state": "FL", "lat": 27.9390, "lng": -82.4740, "base_score": 73},
        ]

    async def _find_similar_opportunities(
        self, idea_description: str, limit: int = 10
    ) -> List[Opportunity]:
        """Find opportunities similar to the idea description"""
        keywords = idea_description.lower().split()[:5]
        
        query = self.db.query(Opportunity)
        
        for keyword in keywords:
            if len(keyword) > 3:
                query = query.filter(
                    Opportunity.title.ilike(f"%{keyword}%") |
                    Opportunity.description.ilike(f"%{keyword}%")
                )
        
        return query.order_by(Opportunity.feasibility_score.desc().nullslast()).limit(limit).all()

    async def _deepseek_pattern_analysis(
        self,
        idea: str,
        similar_opps: List[Opportunity],
        context: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """DeepSeek analysis for pattern recognition"""
        from .ai_orchestrator import ai_orchestrator, AITaskType
        
        data = {
            "idea": idea,
            "similar_opportunities": [
                {"title": o.title, "category": o.category, "score": o.feasibility_score or 0}
                for o in similar_opps[:5]
            ],
            "context": context or {},
        }
        
        result = await ai_orchestrator.process_request(
            AITaskType.OPPORTUNITY_VALIDATION, data
        )
        
        return {
            "patterns_found": len(similar_opps),
            "market_signals": result.get("result", {}) if result.get("processed") else {},
            "category_distribution": self._analyze_categories(similar_opps),
            "average_score": sum(o.feasibility_score or 0 for o in similar_opps) / max(len(similar_opps), 1),
        }

    async def _deepseek_pattern_analysis_parallel(
        self,
        idea: str,
        context: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        DeepSeek pattern analysis that runs in parallel (no dependency on similar_opps).
        Analyzes the idea description directly for online/physical patterns.
        """
        import asyncio
        from .ai_orchestrator import ai_orchestrator, AITaskType
        
        data = {
            "idea": idea,
            "similar_opportunities": [],
            "context": context or {},
            "request": "Analyze this business idea for online vs physical business model patterns.",
        }
        
        try:
            result = await asyncio.wait_for(
                ai_orchestrator.process_request(AITaskType.OPPORTUNITY_VALIDATION, data),
                timeout=30.0
            )
            
            return {
                "patterns_found": 0,
                "market_signals": result.get("result", {}) if result.get("processed") else {},
                "category_distribution": {},
                "average_score": 0,
            }
        except asyncio.TimeoutError:
            logger.warning("DeepSeek pattern analysis timed out after 30s")
            return {
                "patterns_found": 0,
                "market_signals": {},
                "category_distribution": {},
                "average_score": 0,
            }
        except Exception as e:
            logger.warning(f"DeepSeek pattern analysis failed: {e}")
            return {
                "patterns_found": 0,
                "market_signals": {},
                "category_distribution": {},
                "average_score": 0,
            }

    def _calculate_business_type_scores(
        self, pattern_analysis: Dict[str, Any], context: Optional[Dict[str, Any]]
    ) -> tuple:
        """Calculate online vs physical business type scores"""
        online_score = 50
        physical_score = 50
        
        context = context or {}
        
        if context.get("digital_product"):
            online_score += 20
        if context.get("requires_physical_delivery"):
            physical_score += 15
        if context.get("location_dependent"):
            physical_score += 20
        if context.get("remote_interaction"):
            online_score += 15
        if context.get("global_scalability"):
            online_score += 25
        if context.get("physical_inventory"):
            physical_score += 20
        
        categories = pattern_analysis.get("category_distribution", {})
        try:
            tech_score = float(categories.get("Technology", 0) or 0)
        except (ValueError, TypeError):
            tech_score = 0.0
        try:
            local_score = float(categories.get("Local Services", 0) or 0)
        except (ValueError, TypeError):
            local_score = 0.0
        if tech_score > 0.3:
            online_score += 10
        if local_score > 0.3:
            physical_score += 10
        
        online_score = min(100, max(0, online_score))
        physical_score = min(100, max(0, physical_score))
        
        return online_score, physical_score

    def _determine_recommendation(self, online_score: int, physical_score: int) -> str:
        """Determine business type recommendation"""
        difference = abs(online_score - physical_score)
        
        if difference < 15:
            return "HYBRID"
        elif online_score > physical_score:
            return "ONLINE"
        else:
            return "PHYSICAL"

    def _parse_address_components(self, address: str) -> tuple[str, Optional[str]]:
        """
        Parse an address string to extract city and state.
        Handles formats like:
        - "7550 Okeechobee Blvd, West Palm Beach, FL 33411"
        - "West Palm Beach, FL"
        - "Miami, Florida"
        """
        import re
        if not address:
            return "", None
        
        parts = [p.strip() for p in address.split(',')]
        
        if len(parts) >= 2:
            last_part = parts[-1].strip()
            state_zip_match = re.match(r'^([A-Z]{2})\s*\d{0,5}', last_part.upper())
            if state_zip_match:
                state = state_zip_match.group(1)
                city = parts[-2].strip() if len(parts) >= 2 else ""
                return city, state
            
            if len(last_part) == 2 and last_part.upper() in STATE_ABBREVIATIONS.values():
                city = parts[-2].strip() if len(parts) >= 2 else ""
                return city, last_part.upper()
            
            state_lower = last_part.lower()
            if state_lower in STATE_ABBREVIATIONS:
                city = parts[-2].strip() if len(parts) >= 2 else ""
                return city, STATE_ABBREVIATIONS[state_lower]
        
        return parts[0] if parts else address, None

    def _get_city_center_coords(self, city: str, state: Optional[str]) -> Dict[str, float]:
        """
        Get center coordinates for a city. Returns lat/lng dict.
        Uses centralized location_utils for consistent fallback hierarchy.
        """
        from .location_utils import get_location_coords
        
        return get_location_coords(
            city=city,
            state=state,
            context="consultant_studio._get_city_center_coords"
        )

    def _infer_business_category(self, business_description: str) -> str:
        """Infer business category from natural language description"""
        desc_lower = business_description.lower()
        
        hospitality_keywords = ["restaurant", "hotel", "motel", "cafe", "bar", "pub", "bistro", 
                                "inn", "lodge", "resort", "diner", "eatery", "food truck",
                                "catering", "bakery", "pizzeria", "brewery", "winery"]
        retail_keywords = ["shop", "store", "boutique", "outlet", "retail", "market", 
                          "grocery", "pharmacy", "clothing", "electronics", "furniture",
                          "jewelry", "bookstore", "florist", "pet store", "hardware"]
        multifamily_keywords = ["apartment", "multifamily", "housing", "residential", 
                               "condo", "townhouse", "duplex", "rental", "property"]
        service_keywords = ["gym", "fitness", "salon", "spa", "barber", "laundry", 
                           "laundromat", "dry clean", "auto", "repair", "dental",
                           "medical", "clinic", "daycare", "childcare", "veterinary"]
        
        for keyword in hospitality_keywords:
            if keyword in desc_lower:
                return "hospitality"
        
        for keyword in multifamily_keywords:
            if keyword in desc_lower:
                return "multifamily"
        
        for keyword in retail_keywords:
            if keyword in desc_lower:
                return "retail"
        
        for keyword in service_keywords:
            if keyword in desc_lower:
                return "services"
        
        return "specific_business"

    async def _claude_viability_report(
        self,
        idea: str,
        pattern_analysis: Dict[str, Any],
        online_score: int,
        physical_score: int,
        context: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Claude generates comprehensive viability report with timeout protection"""
        import asyncio
        from .ai_orchestrator import ai_orchestrator, AITaskType
        
        data = {
            "idea": idea,
            "pattern_analysis": pattern_analysis,
            "online_score": online_score,
            "physical_score": physical_score,
            "context": context or {},
            "request": "Generate a comprehensive viability analysis with strengths, weaknesses, opportunities, and threats.",
        }
        
        try:
            result = await asyncio.wait_for(
                ai_orchestrator.process_request(AITaskType.MARKET_RESEARCH, data),
                timeout=15.0
            )
            ai_insights = result.get("result", {}) if result.get("processed") else {}
        except asyncio.TimeoutError:
            logger.warning("Claude viability report timed out after 15s")
            ai_insights = {"status": "Analysis in progress"}
        except Exception as e:
            logger.warning(f"Claude viability report failed: {e}")
            ai_insights = {}
        
        return {
            "executive_summary": "Viability analysis based on market patterns and business context.",
            "strengths": ["Market demand validated", "Clear target audience"],
            "weaknesses": ["Competition level unknown", "Requires further market research"],
            "opportunities": ["Growing market segment", "Technology enablers available"],
            "threats": ["Market saturation risk", "Regulatory considerations"],
            "ai_insights": ai_insights,
            "confidence_score": 75,
        }

    async def _claude_viability_report_parallel(
        self,
        idea: str,
        context: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Claude generates viability report in parallel with DeepSeek.
        Does not require pattern_analysis or scores - runs concurrently.
        """
        import asyncio
        from .ai_orchestrator import ai_orchestrator, AITaskType
        
        data = {
            "idea": idea,
            "context": context or {},
            "request": "Generate a comprehensive SWOT viability analysis with strengths, weaknesses, opportunities, and threats for this business idea.",
        }
        
        try:
            result = await asyncio.wait_for(
                ai_orchestrator.process_request(AITaskType.MARKET_RESEARCH, data),
                timeout=30.0
            )
            ai_insights = result.get("result", {}) if result.get("processed") else {}
            
            if ai_insights:
                return {
                    "executive_summary": ai_insights.get("summary", "Viability analysis based on market patterns."),
                    "strengths": ai_insights.get("strengths", ["Market demand validated", "Clear target audience"]),
                    "weaknesses": ai_insights.get("weaknesses", ["Competition level unknown", "Requires further research"]),
                    "opportunities": ai_insights.get("opportunities", ["Growing market segment", "Technology enablers available"]),
                    "threats": ai_insights.get("threats", ["Market saturation risk", "Regulatory considerations"]),
                    "ai_insights": ai_insights,
                    "confidence_score": ai_insights.get("confidence", 75),
                }
        except asyncio.TimeoutError:
            logger.warning("Claude parallel viability report timed out after 30s")
        except Exception as e:
            logger.warning(f"Claude parallel viability report failed: {e}")
        
        return {
            "executive_summary": "Viability analysis based on market patterns and business context.",
            "strengths": ["Market demand validated", "Clear target audience"],
            "weaknesses": ["Competition level unknown", "Requires further market research"],
            "opportunities": ["Growing market segment", "Technology enablers available"],
            "threats": ["Market saturation risk", "Regulatory considerations"],
            "confidence_score": 75,
        }

    async def _search_opportunities(self, filters: Dict[str, Any]) -> List[Opportunity]:
        """Search opportunities based on filters"""
        query = self.db.query(Opportunity)
        
        if filters.get("category"):
            query = query.filter(Opportunity.category == filters["category"])
        if filters.get("min_score"):
            query = query.filter(Opportunity.feasibility_score >= filters["min_score"])
        if filters.get("query"):
            search_term = f"%{filters['query']}%"
            query = query.filter(
                Opportunity.title.ilike(search_term) |
                Opportunity.description.ilike(search_term)
            )
        
        return query.order_by(Opportunity.feasibility_score.desc().nullslast()).limit(50).all()

    async def _detect_trends(
        self, opportunities: List[Opportunity], filters: Dict[str, Any]
    ) -> List[DetectedTrend]:
        """DeepSeek trend detection from opportunities"""
        from .ai_orchestrator import ai_orchestrator, AITaskType
        
        categories = {}
        for opp in opportunities:
            cat = opp.category or "Unknown"
            categories[cat] = categories.get(cat, 0) + 1
        
        top_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)[:5]
        
        trend_names = [f"{cat_name} Growth" for cat_name, _ in top_categories]
        existing_by_name = {
            trend.trend_name: trend
            for trend in self.db.query(DetectedTrend).filter(
                DetectedTrend.trend_name.in_(trend_names)
            ).all()
        }

        trends = []
        for cat_name, count in top_categories:
            trend_name = f"{cat_name} Growth"
            existing = existing_by_name.get(trend_name)
            
            if not existing:
                trend = DetectedTrend(
                    trend_name=trend_name,
                    trend_strength=min(100, count * 10),
                    description=f"Growing opportunities in {cat_name} category",
                    category=cat_name,
                    opportunities_count=count,
                    growth_rate=round(count / max(len(opportunities), 1) * 100, 2),
                    confidence_score=70,
                )
                self.db.add(trend)
                trends.append(trend)
            else:
                existing.opportunities_count = count
                existing.trend_strength = min(100, count * 10)
                trends.append(existing)
        
        self.db.commit()
        
        return trends

    async def _claude_trend_synthesis(
        self,
        opportunities: List[Opportunity],
        trends: List[DetectedTrend],
        filters: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Claude synthesizes trends into actionable insights"""
        from .ai_orchestrator import ai_orchestrator, AITaskType
        
        data = {
            "opportunity_count": len(opportunities),
            "trends": [{"name": t.trend_name, "strength": t.trend_strength} for t in trends],
            "filters": filters,
        }
        
        result = await ai_orchestrator.process_request(
            AITaskType.MARKET_RESEARCH, data
        )
        
        return {
            "summary": f"Analysis of {len(opportunities)} opportunities reveals {len(trends)} key trends.",
            "top_insight": trends[0].trend_name if trends else "No clear trend detected",
            "recommendations": [
                "Focus on high-strength trend categories",
                "Monitor emerging patterns",
                "Consider cross-category opportunities",
            ],
            "ai_synthesis": result.get("result", {}) if result.get("processed") else {},
        }

    async def _deepseek_geo_analysis(
        self,
        city: str,
        business_description: str,
        inferred_category: str,
        params: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Geographic analysis using trade area analyzer for real data"""
        from .trade_area_analyzer import trade_area_analyzer
        
        try:
            # Parse city and state from input like "Miami, Florida"
            parsed_city, parsed_state = parse_city_state(city)
            
            # Use parsed state, or fallback to params if provided
            state = parsed_state or (params.get("state") if params else None)
            
            logger.info(f"[PARSE] City input: '{city}' -> city='{parsed_city}', state='{state}'")
            
            opportunity_data = {
                "id": 0,
                "title": f"{business_description} in {parsed_city}",
                "business_description": business_description,
                "category": inferred_category,
                "location": city,
                "city": parsed_city,
                "region": state,
                "state": state,
                "latitude": params.get("latitude") if params else None,
                "longitude": params.get("longitude") if params else None,
            }
            
            trade_area = await trade_area_analyzer.analyze_async(opportunity_data)
            
            demographics = trade_area.demographics or {}
            competitors = trade_area.competitors or []
            
            radius_miles = trade_area.radius_km * 0.621371
            
            return {
                "market_density": "high" if len(competitors) > 10 else "medium" if len(competitors) > 5 else "low",
                "competition_level": "high" if trade_area.white_space_score < 30 else "moderate" if trade_area.white_space_score < 60 else "low",
                "competitor_count": len(competitors),
                "white_space_score": trade_area.white_space_score,
                "demographics": {
                    "population": demographics.get("population", "N/A"),
                    "median_income": f"${demographics.get('median_income', 0):,}" if demographics.get("median_income") else "N/A",
                    "median_age": demographics.get("median_age", "N/A"),
                    "unemployment_rate": f"{demographics.get('unemployment_rate', 0)}%" if demographics.get("unemployment_rate") else "N/A",
                    "median_home_value": f"${demographics.get('median_home_value', 0):,}" if demographics.get("median_home_value") else "N/A",
                },
                "trade_area_radius_miles": round(radius_miles, 1),
                "competitors": competitors[:20],
                "ai_synthesis": trade_area.ai_synthesis,
            }
            
        except Exception as e:
            logger.warning(f"Trade area analysis failed, using fallback: {e}")
            return {
                "market_density": "medium",
                "competition_level": "moderate",
                "demographics": {
                    "population": "100,000+",
                    "median_income": "$55,000",
                    "growth_trend": "positive",
                },
                "ai_insights": {},
            }

    async def _claude_location_report(
        self,
        city: str,
        business_description: str,
        inferred_category: str,
        geo_analysis: Dict[str, Any],
        params: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Claude generates comprehensive location market report with parallelized AI calls"""
        from .ai_report_generator import ai_report_generator
        import asyncio
        
        try:
            loop = asyncio.get_running_loop()
            
            opportunity_context = {
                "title": f"{business_description} in {city}",
                "category": inferred_category,
                "location": city,
                "description": f"Market opportunity for {business_description} business in {city}",
            }
            
            demographics = geo_analysis.get("demographics", {})
            competitors = geo_analysis.get("competitors", [])
            
            market_insights, competitive_analysis = await asyncio.gather(
                loop.run_in_executor(
                    None,
                    ai_report_generator.generate_market_insights,
                    opportunity_context,
                    demographics,
                    competitors
                ),
                loop.run_in_executor(
                    None,
                    ai_report_generator.generate_competitive_analysis,
                    opportunity_context,
                    competitors
                )
            )
            
            return {
                "executive_summary": f"Market analysis for {business_description} opportunities in {city}.",
                "market_conditions": geo_analysis.get("market_density", "unknown"),
                "white_space_score": geo_analysis.get("white_space_score", 50),
                "key_factors": [
                    "Local economic indicators",
                    "Competition landscape",
                    "Target demographic presence",
                ],
                "market_insights": market_insights,
                "competitive_analysis": competitive_analysis,
            }
            
        except Exception as e:
            logger.warning(f"AI location report failed, using fallback: {e}")
            return {
                "executive_summary": f"Market analysis for {business_description} opportunities in {city}.",
                "market_conditions": geo_analysis.get("market_density", "unknown"),
                "key_factors": [
                    "Local economic indicators",
                    "Competition landscape",
                    "Target demographic presence",
                ],
                "ai_report": {},
            }

    def _generate_quick_synthesis(
        self,
        opportunities: List,
        trends: List,
        filters: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Generate quick synthesis from opportunities and trends without Claude"""
        query = filters.get("query", "business opportunities")
        category = filters.get("category")
        
        top_categories = {}
        for opp in opportunities[:20]:
            cat = getattr(opp, 'category', 'uncategorized')
            top_categories[cat] = top_categories.get(cat, 0) + 1
        
        sorted_categories = sorted(top_categories.items(), key=lambda x: x[1], reverse=True)[:3]
        
        insights = []
        if len(opportunities) > 10:
            insights.append(f"Strong market activity with {len(opportunities)} opportunities identified")
        elif len(opportunities) > 0:
            insights.append(f"Found {len(opportunities)} relevant opportunities")
        
        if trends:
            top_trend = trends[0] if trends else None
            if top_trend:
                insights.append(f"Leading trend: {getattr(top_trend, 'trend_name', 'Emerging markets')}")
        
        if sorted_categories:
            top_cat = sorted_categories[0][0]
            insights.append(f"Most active category: {top_cat}")
        
        return {
            "summary": f"Analysis of {query} opportunities" + (f" in {category}" if category else ""),
            "opportunity_count": len(opportunities),
            "trend_count": len(trends),
            "top_categories": [{"name": cat, "count": count} for cat, count in sorted_categories],
            "key_insights": insights if insights else ["Market data collected", "Review opportunities for details"],
        }

    def _generate_quick_market_report(
        self,
        city: str,
        business_description: str,
        inferred_category: str,
        geo_analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Generate quick market report from DeepSeek geo_analysis data without Claude"""
        demographics = geo_analysis.get("demographics", {})
        competitors = geo_analysis.get("competitors", [])
        market_indicators = geo_analysis.get("market_indicators", {})
        
        competition_level = "low" if len(competitors) < 3 else "moderate" if len(competitors) < 8 else "high"
        try:
            median_income = int(demographics.get("median_income", 50000) or 50000)
        except (ValueError, TypeError):
            median_income = 50000
        try:
            population = int(demographics.get("population", 0) or 0)
        except (ValueError, TypeError):
            population = 0
        
        market_score = 70
        if median_income > 75000:
            market_score += 10
        if population > 100000:
            market_score += 5
        if len(competitors) < 5:
            market_score += 10
        elif len(competitors) > 10:
            market_score -= 5
        
        insights = []
        if median_income > 75000:
            insights.append(f"High income area (${median_income:,} median) supports premium pricing")
        if population > 50000:
            insights.append(f"Large population base ({population:,}) provides customer volume")
        if len(competitors) < 5:
            insights.append(f"Low competition ({len(competitors)} similar businesses) indicates market opportunity")
        elif len(competitors) > 8:
            insights.append(f"Established market ({len(competitors)} competitors) shows proven demand")
        
        return {
            "market_score": min(100, max(0, market_score)),
            "competition_level": competition_level,
            "competitor_count": len(competitors),
            "demographics_summary": {
                "median_income": median_income,
                "population": population,
                "median_age": demographics.get("median_age", 35),
            },
            "key_insights": insights if insights else ["Market analysis available", "Review competitor data for positioning"],
            "recommendation": "favorable" if market_score >= 70 else "moderate" if market_score >= 50 else "challenging",
        }

    def _generate_site_recommendations(
        self,
        geo_analysis: Dict[str, Any],
        market_report: Dict[str, Any],
        business_type: str,
    ) -> List[Dict[str, Any]]:
        """Generate site recommendations based on analysis"""
        base_recommendations = {
            "specific_business": [
                {"type": "Downtown Core", "priority": "high", "reason": "High foot traffic"},
                {"type": "Business District", "priority": "medium", "reason": "Professional clientele"},
            ],
            "retail": [
                {"type": "Shopping Center", "priority": "high", "reason": "Established traffic patterns"},
                {"type": "Main Street", "priority": "medium", "reason": "Local visibility"},
            ],
            "multifamily": [
                {"type": "Urban Infill", "priority": "high", "reason": "Density and accessibility"},
                {"type": "Transit Corridor", "priority": "high", "reason": "Transportation access"},
            ],
            "hospitality": [
                {"type": "Tourism District", "priority": "high", "reason": "Visitor traffic"},
                {"type": "Convention Area", "priority": "medium", "reason": "Business travelers"},
            ],
        }
        
        return base_recommendations.get(business_type, base_recommendations["specific_business"])

    def _generate_cache_key(
        self,
        city: str,
        business_type: str,
        business_subtype: Optional[str],
        params: Optional[Dict[str, Any]],
    ) -> str:
        """Generate unique cache key for location analysis"""
        key_data = f"{city.lower()}:{business_type}:{business_subtype or ''}:{json.dumps(params or {}, sort_keys=True)}"
        return hashlib.sha256(key_data.encode()).hexdigest()[:32]

    def _get_cached_analysis(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Get cached location analysis if not expired"""
        try:
            cached = self.db.query(LocationAnalysisCache).filter(
                LocationAnalysisCache.cache_key == cache_key,
                LocationAnalysisCache.expires_at > datetime.utcnow(),
            ).first()
            
            if cached:
                cached.hit_count = (cached.hit_count or 0) + 1
                cached.updated_at = datetime.utcnow()
                self.db.commit()
                return {
                    "success": True,
                    "city": cached.city,
                    "business_type": cached.business_type,
                    "business_subtype": cached.business_subtype,
                    "geo_analysis": cached.demographic_data or {},
                    "market_report": {"executive_summary": cached.claude_summary},
                    "site_recommendations": cached.site_recommendations or [],
                    "hit_count": cached.hit_count,
                }
        except Exception as e:
            logger.warning(f"Location cache lookup failed: {e}")
            self.db.rollback()

        return None

    async def _cache_analysis(
        self,
        cache_key: str,
        city: str,
        business_type: str,
        business_subtype: Optional[str],
        query_params: Optional[Dict[str, Any]],
        geo_analysis: Dict[str, Any],
        market_report: Dict[str, Any],
        site_recommendations: List[Dict[str, Any]],
    ):
        """Cache location analysis for future use with safe upsert handling"""
        try:
            existing = self.db.query(LocationAnalysisCache).filter(
                LocationAnalysisCache.cache_key == cache_key
            ).first()

            if existing:
                existing.city = city
                existing.business_type = business_type
                existing.business_subtype = business_subtype
                existing.query_params = query_params
                existing.demographic_data = geo_analysis
                existing.market_metrics = geo_analysis.get("demographics")
                existing.claude_summary = market_report.get("executive_summary")
                existing.site_recommendations = site_recommendations
                existing.expires_at = datetime.utcnow() + timedelta(days=self.CACHE_TTL_DAYS)
                existing.updated_at = datetime.utcnow()
                self.db.commit()
                return

            cache_entry = LocationAnalysisCache(
                cache_key=cache_key,
                city=city,
                business_type=business_type,
                business_subtype=business_subtype,
                query_params=query_params,
                demographic_data=geo_analysis,
                market_metrics=geo_analysis.get("demographics"),
                claude_summary=market_report.get("executive_summary"),
                site_recommendations=site_recommendations,
                expires_at=datetime.utcnow() + timedelta(days=self.CACHE_TTL_DAYS),
            )
            self.db.add(cache_entry)
            self.db.commit()
        except IntegrityError:
            self.db.rollback()
            existing = self.db.query(LocationAnalysisCache).filter(
                LocationAnalysisCache.cache_key == cache_key
            ).first()
            if existing:
                existing.city = city
                existing.business_type = business_type
                existing.business_subtype = business_subtype
                existing.query_params = query_params
                existing.demographic_data = geo_analysis
                existing.market_metrics = geo_analysis.get("demographics")
                existing.claude_summary = market_report.get("executive_summary")
                existing.site_recommendations = site_recommendations
                existing.expires_at = datetime.utcnow() + timedelta(days=self.CACHE_TTL_DAYS)
                existing.updated_at = datetime.utcnow()
                self.db.commit()
        except Exception as e:
            logger.warning(f"Failed to cache location analysis: {e}")
            self.db.rollback()

    async def _log_activity(
        self,
        user_id: int,
        session_id: Optional[str],
        path: str,
        action: str,
        payload: Optional[Dict[str, Any]],
        result_summary: str,
        ai_model_used: str,
        processing_time_ms: int,
        tokens_used: Optional[int] = None,
    ):
        """Log consultant activity"""
        activity = ConsultantActivity(
            user_id=user_id,
            session_id=session_id,
            path=path,
            action=action,
            payload=payload,
            result_summary=result_summary,
            ai_model_used=ai_model_used,
            tokens_used=tokens_used,
            processing_time_ms=processing_time_ms,
        )
        self.db.add(activity)
        self.db.commit()

    def _analyze_categories(self, opportunities: List[Opportunity]) -> Dict[str, float]:
        """Analyze category distribution"""
        if not opportunities:
            return {}
        
        categories = {}
        for opp in opportunities:
            cat = opp.category or "Unknown"
            categories[cat] = categories.get(cat, 0) + 1
        
        total = len(opportunities)
        return {k: round(v / total, 2) for k, v in categories.items()}
